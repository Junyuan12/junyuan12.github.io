{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/03/25/hello-world/"},{"title":"Test","text":"a sample code 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&quot;&quot;&quot;Adapted from keras example cifar10_cnn.pyTrain ResNet-18 on the CIFAR10 small images dataset.GPU run command with Theano backend (with TensorFlow, the GPU is automatically used): THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python cifar10.py&quot;&quot;&quot;from __future__ import print_functionfrom keras.datasets import cifar10from keras.preprocessing.image import ImageDataGeneratorfrom keras.utils import np_utilsfrom keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStoppingimport numpy as npimport resnetlr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)early_stopper = EarlyStopping(min_delta=0.001, patience=10)csv_logger = CSVLogger(&apos;resnet18_cifar10.csv&apos;)batch_size = 32nb_classes = 10nb_epoch = 200data_augmentation = True# input image dimensionsimg_rows, img_cols = 32, 32# The CIFAR10 images are RGB.img_channels = 3# The data, shuffled and split between train and test sets:(X_train, y_train), (X_test, y_test) = cifar10.load_data()# Convert class vectors to binary class matrices.Y_train = np_utils.to_categorical(y_train, nb_classes)Y_test = np_utils.to_categorical(y_test, nb_classes)X_train = X_train.astype(&apos;float32&apos;)X_test = X_test.astype(&apos;float32&apos;)# subtract mean and normalizemean_image = np.mean(X_train, axis=0)X_train -= mean_imageX_test -= mean_imageX_train /= 128.X_test /= 128.model = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)model.compile(loss=&apos;categorical_crossentropy&apos;, optimizer=&apos;adam&apos;, metrics=[&apos;accuracy&apos;])if not data_augmentation: print(&apos;Not using data augmentation.&apos;) model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, validation_data=(X_test, Y_test), shuffle=True, callbacks=[lr_reducer, early_stopper, csv_logger])else: print(&apos;Using real-time data augmentation.&apos;) # This will do preprocessing and realtime data augmentation: datagen = ImageDataGenerator( featurewise_center=False, # set input mean to 0 over the dataset samplewise_center=False, # set each sample mean to 0 featurewise_std_normalization=False, # divide inputs by std of the dataset samplewise_std_normalization=False, # divide each input by its std zca_whitening=False, # apply ZCA whitening rotation_range=0, # randomly rotate images in the range (degrees, 0 to 180) width_shift_range=0.1, # randomly shift images horizontally (fraction of total width) height_shift_range=0.1, # randomly shift images vertically (fraction of total height) horizontal_flip=True, # randomly flip images vertical_flip=False) # randomly flip images # Compute quantities required for featurewise normalization # (std, mean, and principal components if ZCA whitening is applied). datagen.fit(X_train) # Fit the model on the batches generated by datagen.flow(). model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size), steps_per_epoch=X_train.shape[0] // batch_size, validation_data=(X_test, Y_test), epochs=nb_epoch, verbose=1, max_q_size=100, callbacks=[lr_reducer, early_stopper, csv_logger])","link":"/2019/03/25/test/"}],"tags":[],"categories":[]}